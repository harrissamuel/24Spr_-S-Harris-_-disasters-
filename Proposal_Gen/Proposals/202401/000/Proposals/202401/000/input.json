{
  "Version": "000",
  "Year": "2024",
  "Semester": "Spring",
  "project_name": "Mapping and Predicting Natural Disasters and Their Cost",
  "Objective": " \n            The first goal of this project is to develop interactive map(s) of disaster data using FEMA (Federal Emergency\n            Management Agency) time series data of over 4800+ data points labeled by state from 1953 to 2023. The second \n            goal is to merge the FEMA data with NOAA (National Oceanic and Atmospheric Administration) historical state \n            temperature and precipitation data and disaster cost data to build a predictive model. Ideally, two predictive \n            models will use features from historical climate data to try and predict disaster classification and \n            disaster cost. The mapping of historical data and effective predictive models will help state officials use \n            predicted climate data to produce relevant disaster risk predictions. The main objective is for this project to \n            underscore past disasters while also helping officials make data-driven preparations for future climate disasters.\n            ",
  "Dataset": "\n           At this initial stage, three US government datasets will be used: time series data containing individual natural \n           disasters from FEMA, tabular data containing average temperature and precipitation statistics by state from the NOAA, \n           and billion-dollar disaster cost data by state from the NOAA. The first dataset is \u201cFEMA Web Disaster Declarations\u201d \n           and contains \u201ca list of FEMA declaration types and the types of assistance authorized.\u201d More specifically, it has \n           4800 rows containing columns like \u2018Disaster Name\u2019, \u2018Declaration Date,\u2019 \u2018Incident Type\u2019, and \u2018State Name.\u2019 Dates span \n           from 1953 to 2023. This data will be foundational for initial exploratory data analysis and mapping of historical \n           disasters in the US by state. The second dataset is statewide NOAA climate data containing columns like temperature \n           and precipitation from 1895 to 2023. This data can be specified so it is by month and state. I plan to extract average \n           climate data from at least five different states every month since 1953. The next stage with this data is to merge it \n           with the FEMA dataset and obtain climate data matched with disaster data to build a predictive model for disaster type. \n           The initial idea is to use a random forest algorithm to build the classification model. Finally, the last dataset is \n           statewide NOAA disaster cost data which contains disaster cost data by month and year from 1980-2023. It also contains \n           columns for the name of the disaster and the type of disaster. Again, I plan to merge this data with the NOAA climate data \n           and obtain disaster cost data matched with climatic data to build a predictive model for disaster cost. The initial idea \n           is to use regression to build this predictive model for disaster cost. Links to the data can be found below.\n\n           Dataset 1- Disaster Time Series: \n           https://www.fema.gov/openfema-data-page/fema-web-disaster-declarations-v1\n           Dataset 2- Climatic Data: \n           https://www.ncei.noaa.gov/access/monitoring/climate-at-a-glance/statewide/time-series/41/tavg/1/12/1895-2023\n           Dataset 3- Disaster Cost Data:\n           https://www.ncei.noaa.gov/access/billions/state-summary/TX\n            ",
  "Rationale": "\n             The rationale of this project is to provide a complex understanding of past disasters in US states and help \n             specified states stay prepared for natural disasters in the future. This project should provide an idea \n             about the quantity of funds and resources that should be devoted to natural disasters as human-caused climate\n             change continues. \n            ",
  "Approach": "\n            I plan on approaching this capstone through several steps.  \n\n            1. Data cleaning/wrangling and exploratory data analysis with FEMA data (R/Python) \n            2. Mapping and plotting FEMA data (R)   \n            3. Data wrangling and cleaning to combine data sets (Python)\n            4. Build/test models + feature engineering (ML with scikit-learn in Python)\n            5. Produce final predictive results and combine them with historical results  \n            ",
  "Timeline": "\n            This a rough time line for this project:  \n\n            - (2 Weeks) Data Processing and Analysis\n            - (2 Weeks) Mapping and Plotting\n            - (2 Weeks) Combine Datasets via Data Wrangling\n            - (5 Weeks) Modeling + Feature Engineering \n            - (1 Weeks) Combining Results  \n            - (1 Weeks) Writing Up a paper and submission\n            - (1 Weeks) Final Presentation  \n\n            ",
  "Expected Number Students": "\n            The expected number of students is one.  \n            ",
  "Possible Issues": "\n            The challenge will be data wrangling and effectively combining the datasets using dates and locations. \n            More challenges will exist in creating good predictive models since there are a limited number of \n            climatic features and these features are not guaranteed to be good predictors.\n            ",
  "Proposed by": "Samuel Harris",
  "Proposed by email": "samharris@gwu.edu",
  "instructor": "Edwin Lo",
  "instructor_email": "edwinlo@gwu.edu",
  "github_repo": "https://github.com/harrissamuel/24Spr_-S-Harris-_-projectName-.git"
}